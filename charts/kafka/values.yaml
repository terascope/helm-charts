# ------------------------------------------------------------------------------
# Kafka:
# ------------------------------------------------------------------------------

## The StatefulSet installs 3 pods by default
replicas: 3

## The kafka image repository
image: "apache/kafka"

## The kafka image tag
imageTag: "3.7.1"

## Specify a imagePullPolicy
## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
imagePullPolicy: "IfNotPresent"


## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
resources: {}
  # limits:
  #   cpu: 200m
  #   memory: 1536Mi
  # requests:
  #   cpu: 100m
  #   memory: 1024Mi
kafkaHeapOptions: "-Xmx1G -Xms1G"

## Optional Container Security context
securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

## The StatefulSet Update Strategy which Kafka will use when changes are applied.
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
updateStrategy:
  type: "OnDelete"

## Start and stop pods in Parallel or OrderedReady (one-by-one.)  Note - Can not change after first release.
## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
podManagementPolicy: Parallel

## Useful if using any custom authorizer
## Pass in some secrets to use (if required)
# secrets:
# - name: myKafkaSecret
#   keys:
#     - username
#     - password
#   # mountPath: /opt/kafka/secret

## The subpath within the Kafka container's PV where logs will be stored.
## This is combined with `persistence.mountPath`, to create, by default: /opt/kafka/data/logs
logSubPath: "logs"

## Use an alternate scheduler, e.g. "stork".
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
# schedulerName:

## Use an alternate serviceAccount
## Useful when using images in custom repositories
# serviceAccountName:

## Set a pod priorityClassName
# priorityClassName: high-priority

## Pod scheduling preferences (by default keep pods within a release on separate nodes).
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## By default we don't set affinity
affinity: {}
## Alternatively, this typical example defines:
## antiAffinity (to keep Kafka pods on separate pods)
# affinity:
#   podAntiAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#     - labelSelector:
#         matchExpressions:
#         - key: app
#           operator: In
#           values:
#           - kafka
#       topologyKey: "kubernetes.io/hostname"

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

topologySpreadConstraints: []

## Readiness probe config.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
##
readinessProbe:
  initialDelaySeconds: 60
  periodSeconds: 60
  timeoutSeconds: 10
  successThreshold: 1
  failureThreshold: 10

livenessProbe:
  initialDelaySeconds: 480
  periodSeconds: 60
  timeoutSeconds: 30
  successThreshold: 1
  failureThreshold: 10

## Period to wait for broker graceful shutdown (sigterm) before pod is killed (sigkill)
## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination
## ref: https://kafka.apache.org/10/documentation.html#brokerconfigs controlled.shutdown.*
terminationGracePeriodSeconds: 30

# Tolerations for nodes that have taints on them.
# Useful if you want to dedicate nodes to just run kafka
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []
# tolerations:
# - key: "key"
#   operator: "Equal"
#   value: "value"
#   effect: "NoSchedule"

## Service
##
service:
  broker:
    port: 9092
    targetPort: kafka
    
## Headless service.
##
headless:
  # annotations:
  broker:
    plaintext:
      port: 9092
      # targetPort:
    ssl:
      port: 9094
      # targetPort:
  controller:  
    port: 9093
    # targetPort:

clusterDomain: cluster.local

## extraInitContainers is a list of init containers. Specified as a YAML list.
#
extraInitContainers: 
  ## This example downloads the JMX Exporter Java agent to the /var/lib/prometheus folder,
  ## allowing the use of the agent by setting prometheus.agent.enabled to true.
  # - name: jmx-agent
  #   image: "curlimages/curl:latest"
  #   securityContext:
  #     runAsUser: 0
  #   command: [sh, -c]
  #   args:
  #     - "mkdir -p /var/lib/prometheus && \
  #       wget https://github.com/prometheus/jmx_exporter/releases/download/1.1.0/jmx_prometheus_javaagent-1.1.0.jar -O /var/lib/prometheus/jmx_prometheus_javaagent-1.1.0.jar && \
  #       chmod +x /var/lib/prometheus/jmx_prometheus_javaagent-1.1.0.jar && \
  #       ls -la /var/lib/prometheus"
  #   volumeMounts:
  #     - name: jmx
  #       mountPath: /var/lib/prometheus

## extraContainers is a list of sidecar containers. Specified as a YAML list.
#
extraContainers: null

extraVolumes:
  ## example volume to supply ssl certs
  # - name: certs
  #   hostPath:
  #     path: /certs
  #     type: Directory

extraVolumeMounts:
  ## example volumeMount to supply ssl certs
  # - mountPath: /etc/kafka/secrets
  #   name: certs

## External access.
##
external:
  enabled: false
  # type can be either NodePort or LoadBalancer
  type: NodePort
  # externalTrafficPolicy can be either Cluster or Local
  externalTrafficPolicy: Local
  # annotations:
  #  service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
  # Labels to be added to external services
  # labels:
  #  aLabel: "value"
  dns:
    useInternal: false
    useExternal: true
  # If using external service type LoadBalancer and external dns, set distinct to true below.
  # This creates an A record for each statefulset pod/broker. You should then map the
  # A record of the broker to the EXTERNAL IP given by the LoadBalancer in your DNS server.
  distinct: false
  servicePort: 19092
  firstListenerPort: 31090
  domain: cluster.local
  loadBalancerIP: []
  loadBalancerSourceRanges: []
  init:
    image: "lwolf/kubectl_deployer"
    imageTag: "0.4"
    imagePullPolicy: "IfNotPresent"

# Annotation to be added to Kafka pods
podAnnotations: {}

# Labels to be added to Kafka pods
podLabels: {}
  # service: broker
  # team: developers

podDisruptionBudget: {}
  # maxUnavailable: 1  # Limits how many Kafka pods may be unavailable due to voluntary disruptions.

## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
## here in map format, as defined in the official docs.
## ref: https://kafka.apache.org/documentation/#brokerconfigs
##
configurationOverrides: {}
  # "auto.leader.rebalance.enable": true
  # "auto.create.topics.enable": true
  # "controlled.shutdown.enable": true
  # "controlled.shutdown.max.retries": 100

  ## Options required for external access via NodePort
  ## ref:
  ## - http://kafka.apache.org/documentation/#security_configbroker
  ## - https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
  ##
  ## Setting "advertised.listeners" here appends to "PLAINTEXT://${POD_IP}:9092,", ensure you update the domain
  ## If external service type is Nodeport:
  # "advertised.listeners": |-
  #   EXTERNAL://kafka.cluster.local:$((31090 + ${KAFKA_NODE_ID}))
  ## If external service type is LoadBalancer and distinct is true:
  # "advertised.listeners": |-
  #   EXTERNAL://kafka-$((${KAFKA_NODE_ID})).cluster.local:19092
  ## If external service type is LoadBalancer and distinct is false:
  # "advertised.listeners": |-
  #   EXTERNAL://${LOAD_BALANCER_IP}:31090
  ## Uncomment to define the EXTERNAL Listener protocol
  # "listener.security.protocol.map": |-
  #   PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT

  ## SSL/TLS encryption
  ## Documentation: https://kafka.apache.org/documentation/#security
  ## advertised.listeners here are appended to the defaults, not overwritten
  # advertised.listeners: "TLS://kafka-headless.default.svc.cluster.local:9094"
  # listener.security.protocol.map: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,TLS:SSL"
  # listeners: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,TLS://0.0.0.0:9094"
  # security.protocol: "ssl"
  # ssl.client.auth: "none"
  #
  ## Options to enable SSL using PEM format
  ## Provide either ssl.keystore.location & ssl.truststore.location 
  ## or ssl.keystore.key & ssl.keystore.certificate.chain
  # ssl.keystore.certificate.chain: |
  #   -----BEGIN CERTIFICATE-----\  # server cert
  #   MIIEyDCCAzCgAwIBAgIRAO8MGa5Gv9KBRb7D0Rcyj24wDQYJKoZIhvcNAQELBQAw\
  #   ...
  #   xnwiB0bI7V6Kf5KVgisZDtJF8APTfHy4ey3mXA==\
  #   -----END CERTIFICATE-----\ # intermediate certs would come next
  #   -----BEGIN CERTIFICATE-----\ # root cert
  #   MIIFJzCCA4+gAwIBAgIQX6DM59eAmZLzzdoyD0jbtDANBgkqhkiG9w0BAQsFADCB\
  #   ...
  #   R4eNRWMls7ceteGynZLUL0LULwW8Wio8w3Ht\
  #   -----END CERTIFICATE-----
  # ssl.keystore.key: |
  #   -----BEGIN PRIVATE KEY-----\
  #   MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCyI8ugVF4Og0Os\
  #   ...
  #   AdkIu2G1Xt6aidSQX8P5lxE1XadgO8b1JC8v86W4wzXnIZXZP/PE5U3w6eDdAI9w\
  #   X3ENvVkJ02bIDoRJ1oHchbHA\
  #   -----END PRIVATE KEY----
  # ssl.keystore.location: "/etc/kafka/secrets/broker-keypair.pem" # key followed by cert
  # ssl.keystore.type: "PEM"
  # ssl.truststore.location: "/etc/kafka/secrets/CAs/rootCA.pem"
  # ssl.truststore.type: "PEM"
  # inter.broker.listener.name: "PLAINTEXT"
  # security.inter.broker.protocol: "SSL"

## set extra ENVs
#   key: "value"
envOverrides: {}


## A collection of additional ports to expose on brokers (formatted as normal containerPort yaml)
# Useful when the image exposes metrics (like prometheus, etc.) through a javaagent instead of a sidecar
additionalPorts: {}

## Persistence configuration. Specify if and how to persist data to a persistent volume.
##
persistence:
  enabled: true

  ## The size of the PersistentVolume to allocate to each Kafka Pod in the StatefulSet. For
  ## production servers this number should likely be much larger.
  ##
  size: "1Gi"

  ## The location within the Kafka container where the PV will mount its storage and Kafka will
  ## store its logs.
  ##
  mountPath: "/opt/kafka/data"

  ## Kafka data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass:

jmx:
  ## Rules to apply to the Prometheus JMX Exporter.  Note while lots of stats have been cleaned and exposed,
  ## there are still more stats to clean up and expose, others will never get exposed.  They keep lots of duplicates
  ## that can be derived easily.  The configMap in this chart cleans up the metrics it exposes to be in a Prometheus
  ## format, eg topic, broker are labels and not part of metric name. Improvements are gladly accepted and encouraged.
  configMap:

    ## Allows disabling the default configmap, note a configMap is needed
    enabled: true

    ## Allows setting values to generate confimap
    ## To allow all metrics through (warning its crazy excessive) comment out below `overrideConfig` and set
    ## `whitelistObjectNames: []`
    overrideConfig:
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      ssl: false
      whitelistObjectNames: ["kafka.controller:*","kafka.server:*","java.lang:*","kafka.network:*","kafka.log:*", "jvm:*"]
      rules:
      - pattern: kafka.server<type=(BrokerTopicMetrics), name=(BytesInPerSec|BytesOutPerSec|MessagesInPerSec|Failed.+), topic=(.+)><>Count
        name: kafka_server_$1_$2_total
        labels:
          topic: "$3"
        type: COUNTER

      # Node-level replica stats
      - pattern: kafka.server<type=(ReplicaManager), name=(UnderReplicatedPartitions|UnderMinIsrPartitionCount|OfflineReplicaCount|LeaderCount|PartitionCount)><>Value
        name: kafka_server_$1_$2_value
        type: GAUGE
      - pattern: kafka.server<type=(ReplicaManager), name=(IsrExpandsPerSec|IsrShrinksPerSec)><>Value
        name: kafka_server_$1_$2_total
        type: COUNTER

      # Node-level replica lag. This could also be tracked at partition-level granularity, but this would
      # get extremely expensive given the number of Kafka nodes and partitions
      - pattern: kafka.server<type=(ReplicaFetcherManager), name=(MaxLag), clientId=(Replica)><>Value
        name: kafka_server_$1_$2_value
        type: GAUGE

      # Request stats
      - pattern: kafka.network<type=(RequestMetrics), name=(RequestsPerSec|TotalTimeMs|RequestQueueTimeMs|LocalTimeMs|RemoteTimeMs|ResponseQueueTimeMs|ResponseSendTimeMs), request=(Produce|FetchConsumer|FetchFollower)><>Count
        name: kafka_network_$1_$2_total
        labels:
          request: "$3"
        type: COUNTER

      # Controller stats
      - pattern: kafka.controller<type=(ControllerStats), name=(LeaderElectionRateAndTimeMs|UncleanLeaderElectionsPerSec)><>Count
        name: kafka_controller_$1_$2_total
        type: COUNTER
      - pattern: kafka.controller<type=(KafkaController), name=(ActiveControllerCount|OfflinePartitionsCount)><>Value
        name: kafka_controller_$1_$2_value
        type: GAUGE

      - pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
        name: kafka_$1_$2_$3
        type: GAUGE
        labels:
          quantile: "0.$4"

      - pattern: kafka.log<type=Log, name=(.+), topic=(.+), partition=(.+)><>Value
        name: kafka_log_$1
        type: COUNTER
        labels:
          topic: "$2"
          partition: "$3"

    ## If you would like to supply your own ConfigMap for JMX metrics, supply the name of that
    ## ConfigMap as an `overrideName` here.
    overrideName: ""

  ## Port the jmx metrics are exposed in native jmx format, not in Prometheus format
  port: 5555

  remote:
    enabled: true
    localOnly: false
    ssl: false
    authenticate: false

  ## JMX Whitelist Objects, can be set to control which JMX metrics are exposed.  Only whitelisted
  ## values will be exposed via JMX Exporter.  They must also be exposed via Rules.  To expose all metrics
  ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`
  ## (2) commented out above `overrideConfig`.
  whitelistObjectNames:  # []
  - kafka.controller:*
  - kafka.server:*
  - java.lang:*
  - kafka.network:*
  - kafka.log:*

## Prometheus Exporters / Metrics
##
prometheus:
  ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics
  # NOTE: This jmx exporter has not been used, and the exporter running
  #       in the kafka broker container is the preferred method.
  jmx:
    enabled: false

    ## The image to use for the metrics collector
    image: docker.io/solsson/kafka-prometheus-jmx-exporter@sha256

    ## The image tag to use for the metrics collector
    imageTag: 70852d19ab9182c191684a8b08ac831230006d82e65d1db617479ea27884e4e8

    ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
    interval: 10s

    ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 10s

    ## Port jmx-exporter exposes Prometheus format metrics to scrape
    port: 5556

    resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 1Gi
      # requests:
      #   cpu: 100m
      #   memory: 100Mi

  ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter
  kafka:
    enabled: false

    ## The image to use for the metrics collector
    image: docker.io/danielqsj/kafka-exporter

    ## The image tag to use for the metrics collector
    imageTag: v1.8.0

    ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
    interval: 10s

    ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 10s

    ## Port kafka-exporter exposes for Prometheus to scrape metrics
    port: 9308

    ## Resource limits
    resources: {}
    #      limits:
    #        cpu: 200m
    #        memory: 1Gi
    #      requests:
    #        cpu: 100m
    #        memory: 100Mi

    # Tolerations for nodes that have taints on them.
    # Useful if you want to dedicate nodes to just run kafka-exporter
    # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # tolerations:
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## By default we don't set affinity
    affinity: {}
    ## Alternatively, this typical example defines:
    ## affinity (to encourage Kafka Exporter pods to be collocated with Kafka pods)
    # affinity:
    #   podAffinity:
    #     preferredDuringSchedulingIgnoredDuringExecution:
    #      - weight: 50
    #        podAffinityTerm:
    #          labelSelector:
    #            matchExpressions:
    #            - key: app
    #              operator: In
    #              values:
    #                - kafka
    #          topologyKey: "kubernetes.io/hostname"

    ## Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}

  ## WARNING: this feature is not enabled as there is no public image
  kafkaTopicUsage:
    enabled: false # Do Not Enable
    image: # There is no public image for this exporter currently
    imageTag: 0.0.3
    interval: 60s
    kafkaInterval: 6000
    port: 3000
    resources: {}

  operator:
    ## Are you using Prometheus Operator?
    enabled: false

    serviceMonitor:
      # Namespace in which to install the ServiceMonitor resource.
      namespace: monitoring
      # Use release namespace instead
      releaseNamespace: false

      ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
      ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
      ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
      selector:
        prometheus: kube-prometheus

      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: instance
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: kafka_node
        - sourceLabels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          regex: kafka-(.*)
          replacement: $1
          targetLabel: kafka_cluster
      metricRelabelings:
        - action: drop
          sourceLabels: [topic]
          regex: (test.*|temp.*|__consumer.*)
        - action: drop
          sourceLabels: [kafka_topic]
          regex: (test.*|temp.*|__consumer.*)
    prometheusRule:
      ## Add Prometheus Rules?
      enabled: false

      ## Namespace in which to install the PrometheusRule resource.
      namespace: monitoring
      # Use release namespace instead
      releaseNamespace: false

      ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
      ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
      ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
      selector:
        prometheus: kube-prometheus

      ## Some example rules.
      ## e.g. max(kafka_controller_kafkacontroller_activecontrollercount_value{service="my-kafka-release"}) by (service) < 1
      rules:
      # - alert: KafkaNoActiveControllers
      #   annotations:
      #     message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is less than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
      #   expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) < 1
      #   for: 5m
      #   labels:
      #     severity: critical
      # - alert: KafkaMultipleActiveControllers
      #   annotations:
      #     message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is greater than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
      #   expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) > 1
      #   for: 5m
      #   labels:
      #     severity: critical
  ## Prometheus Java Agent JMX Exporter
  agent:
    enabled: false

    ## https://github.com/prometheus/jmx_exporter/releases
    version: 1.1.0

    ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
    interval: 30s

    ## Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 30s

    ## Port agent exposes for Prometheus to scrape metrics
    port: 5557

    ## Custom label selector
    selector: {}

## Kafka Config job configuration
##
configJob:
  ## Specify the number of retries before considering kafka-config job as failed.
  ## https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy
  backoffLimit: 6

## Topic creation and configuration.
## The job will be run on a deployment only when the config has been changed.
## - If 'partitions' and 'replicationFactor' are specified we create the topic (with --if-not-exists.)
## - If 'partitions', 'replicationFactor' and 'reassignPartitions' are specified we reassign the partitions to
## increase the replication factor of an existing topic.
## - If 'partitions' is specified we 'alter' the number of partitions. This will
## silently and safely fail if the new setting isn’t strictly larger than the old (i.e. a NOOP.) Do be aware of the
## implications for keyed topics (ref: https://docs.confluent.io/current/kafka/post-deployment.html#admin-operations)
## - If 'defaultConfig' is specified it's deleted from the topic configuration. If it isn't present,
## it will silently and safely fail.
## - If 'config' is specified it's added to the topic configuration.
##
## Note: To increase the 'replicationFactor' of a topic, 'reassignPartitions' must be set to true (see above).
##
## **WARNING**: this feature has not been implemented 
topics: []
  # - name: myExistingTopicConfig
  #   config: "cleanup.policy=compact,delete.retention.ms=604800000"
  # - name: myExistingTopicReassignPartitions
  #   partitions: 8
  #   replicationFactor: 5
  #   reassignPartitions: true
  # - name: myExistingTopicPartitions
  #   partitions: 8
  # - name: myNewTopicWithConfig
  #   partitions: 8
  #   replicationFactor: 3
  #   defaultConfig: "segment.bytes,segment.ms"
  #   config: "cleanup.policy=compact,delete.retention.ms=604800000"
  # - name: myAclTopicPartitions
  #   partitions: 8
  #   acls:
  #     - user: read
  #       operations: [ Read ]
  #     - user: read_and_write
  #       operations:
  #         - Read
  #         - Write
  #     - user: all
  #       operations: [ All ]

## Enable/disable the chart's tests.  Useful if using this chart as a dependency of
## another chart and you don't want these tests running when trying to develop and
## test your own chart.
testsEnabled: true
