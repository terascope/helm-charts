# ------------------------------------------------------------------------------
# Kafka:
# ------------------------------------------------------------------------------

# -- Kafka Brokers created by StatefulSet
replicas: 3

# -- Kafka Container image name
image: "apache/kafka"

# -- Kafka Container image tag
imageTag: "3.7.1"

# -- Kafka Container imagePullPolicy
# ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
imagePullPolicy: "IfNotPresent"

# -- Kafka resource requests and limits
# ref: http://kubernetes.io/docs/user-guide/compute-resources/
resources: {}
  # limits:
  #   cpu: 200m
  #   memory: 1536Mi
  # requests:
  #   cpu: 100m
  #   memory: 1024Mi

# -- Kafka broker JVM heap options
kafkaHeapOptions: "-Xmx1G -Xms1G"

# -- Optional Kafka Container Security context
securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

# -- The StatefulSet Update Strategy which Kafka will use when changes are applied.
# ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
updateStrategy:
  type: "OnDelete"

# -- Start and stop pods in Parallel or OrderedReady (one-by-one.)  Note - Can not change after first release.
# ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
podManagementPolicy: Parallel

## Useful if using any custom authorizer
# -- Pass any secrets to the kafka pods. Each secret will be passed as an environment variable by default.
# The secret can also be mounted to a specific path if required. Environment variable names are generated as:
# `<secretName>_<secretKey>` (All upper case)
secrets: {}
# - name: myKafkaSecret
#   keys:
#     - username
#     - password
#   # mountPath: /opt/kafka/secret

# -- The subpath within the Kafka container's PV where logs will be stored.
# This is combined with `persistence.mountPath`, to create, by default: /opt/kafka/data/logs
logSubPath: "logs"

# -- Name of Kubernetes scheduler (other than the default), e.g. "stork".
# ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
schedulerName:

# -- Name of Kubernetes serviceAccount. serviceAccount
# Useful when using images in custom repositories
serviceAccountName:

# -- Name of Kubernetes Pod PriorityClass.
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName:

## Pod scheduling preferences (by default keep pods within a release on separate nodes).
## By default we don't set affinity
# -- Defines affinities and anti-affinities for pods as defined in:
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity preferences
affinity: {}
## Alternatively, this typical example defines:
## antiAffinity (to keep Kafka pods on separate pods)
# affinity:
#   podAntiAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#     - labelSelector:
#         matchExpressions:
#         - key: app
#           operator: In
#           values:
#           - kafka
#       topologyKey: "kubernetes.io/hostname"

# -- Node labels for pod assignment
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

# -- Control how Pods are spread across your cluster
topologySpreadConstraints: []

## Readiness probe config.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
##
readinessProbe:
  # -- Number of seconds before probe is initiated.
  initialDelaySeconds: 60
  # -- How often (in seconds) to perform the probe.
  periodSeconds: 60
  # -- Number of seconds after which the probe times out.
  timeoutSeconds: 10
  # -- Minimum consecutive successes for the probe to be considered successful after having failed.
  successThreshold: 1
  # -- After the probe fails this many times, pod will be marked Unready.
  failureThreshold: 10

livenessProbe:
  # -- Number of seconds before probe is initiated.
  initialDelaySeconds: 480
  # -- How often (in seconds) to perform the probe.
  periodSeconds: 60
  # -- Number of seconds after which the probe times out.
  timeoutSeconds: 30
  # -- Minimum consecutive successes for the probe to be considered successful after having failed.
  successThreshold: 1
  # -- After the probe fails this many times, pod will be marked Unready.
  failureThreshold: 10

## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination
## ref: https://kafka.apache.org/10/documentation.html#brokerconfigs controlled.shutdown.*
# -- Period to wait for broker graceful shutdown (sigterm) before pod is killed (sigkill)
terminationGracePeriodSeconds: 600

# -- Tolerations for nodes that have taints on them.
# Useful if you want to dedicate nodes to just run kafka
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []
# tolerations:
# - key: "key"
#   operator: "Equal"
#   value: "value"
#   effect: "NoSchedule"

## Service
##
service:
  broker:
    # -- Broker port to be used for the service.
    port: 9092
    # -- Broker target port to be used for the service.
    targetPort: kafka

## Headless service.
##
headless:
  # -- List of annotations for the headless service.
  # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
  annotations: []
  broker:
    plaintext:
      # -- Broker plaintext port to be used for the headless service.
      port: 9092
      # -- Broker plaintext target port to be used for the headless service. This is not a required value.
      targetPort: null
    ssl:
      # -- Broker ssl port to be used for the headless service.
      port: 9094
      # -- Broker ssl target port to be used for the headless service. This is not a required value.
      targetPort: null
  controller:
    # -- Controller port to be used for the headless service.
    port: 9093
    # -- Controller target port to be used for the headless service. This is not a required value.
    targetPort: null

# -- Domain in which to advertise Kafka listeners
clusterDomain: cluster.local

# -- Additional Kafka init containers. Specified as a YAML list.
#
extraInitContainers:
  ## This example downloads the JMX Exporter Java agent to the /var/lib/prometheus folder,
  ## allowing the use of the agent by setting prometheus.agent.enabled to true.
  # - name: jmx-agent
  #   image: "curlimages/curl:latest"
  #   securityContext:
  #     runAsUser: 0
  #   command: [sh, -c]
  #   args:
  #     - "mkdir -p /var/lib/prometheus && \
  #       wget https://github.com/prometheus/jmx_exporter/releases/download/1.1.0/jmx_prometheus_javaagent-1.1.0.jar -O /var/lib/prometheus/jmx_prometheus_javaagent-1.1.0.jar && \
  #       chmod +x /var/lib/prometheus/jmx_prometheus_javaagent-1.1.0.jar && \
  #       ls -la /var/lib/prometheus"
  #   volumeMounts:
  #     - name: jmx
  #       mountPath: /var/lib/prometheus

# -- Additional Kafka sidecar containers. Specified as a YAML list.
#
extraContainers: null

# -- Additional Kafka volumes. Specified as a YAML list.
extraVolumes:
  ## example volume to supply ssl certs
  # - name: certs
  #   hostPath:
  #     path: /certs
  #     type: Directory

# -- Additional Kafka volume mounts. Specified as a YAML list.
extraVolumeMounts:
  ## example volumeMount to supply ssl certs
  # - mountPath: /etc/kafka/secrets
  #   name: certs

## External access.
##
external:
  # -- If True, exposes Kafka brokers via NodePort (PLAINTEXT by default)
  enabled: false
  # -- Service Type: can be either NodePort or LoadBalancer
  type: NodePort
  # -- Service desires to route external traffic to node-local(Local) or cluster-wide(Cluster) endpoints.
  externalTrafficPolicy: Local
  # -- Additional annotations for the external service.
  annotations: {}
  #  service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
  # Additional labels for the external service.
  labels: {}
  #  aLabel: "value"
  dns:
    # -- If True, add Annotation for internal DNS service
    useInternal: false
    # -- If True, add Annotation for external DNS service
    useExternal: true
  # -- If using external service type LoadBalancer and external dns, set distinct to true below.
  # This creates an A record for each statefulset pod/broker. You should then map the
  # A record of the broker to the EXTERNAL IP given by the LoadBalancer in your DNS server.
  distinct: false
  # -- TCP port configured at external services (one per pod) to relay from NodePort to the external listener port.
  servicePort: 19092
  # -- TCP port which is added pod index number to arrive at the port used for NodePort and external listener port.
  firstListenerPort: 31090
  # -- Domain in which to advertise Kafka external listeners.
  domain: cluster.local
  # -- Add Static IP to the type Load Balancer. Depends on the provider if enabled
  loadBalancerIP: []
  # -- Add IP ranges that are allowed to access the Load Balancer.
  loadBalancerSourceRanges: []
  init:
    # -- Init image
    image: "lwolf/kubectl_deployer"
    # -- Init image tag
    imageTag: "0.4"
    # -- Init image pull policy
    imagePullPolicy: "IfNotPresent"

# -- Annotation to be added to Kafka pods
podAnnotations: {}

# -- Labels to be added to Kafka pods
podLabels: {}
  # service: broker
  # team: developers

# -- Define a Disruption Budget for the Kafka Pods
podDisruptionBudget: {}
  # maxUnavailable: 1  # Limits how many Kafka pods may be unavailable due to voluntary disruptions.

# -- Specify any Kafka configuration setting overrides you would like set on the StatefulSet
# here in map format, as defined in the official docs.
# ref: https://kafka.apache.org/documentation/#brokerconfigs
#
configurationOverrides: {}
  # "auto.leader.rebalance.enable": true
  # "auto.create.topics.enable": true
  # "controlled.shutdown.enable": true
  # "controlled.shutdown.max.retries": 100

  ## Options required for external access via NodePort
  ## ref:
  ## - http://kafka.apache.org/documentation/#security_configbroker
  ## - https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
  ##
  ## Setting "advertised.listeners" here appends to "PLAINTEXT://${POD_IP}:9092,", ensure you update the domain
  ## If external service type is Nodeport:
  # "advertised.listeners": |-
  #   EXTERNAL://kafka.cluster.local:$((31090 + ${KAFKA_NODE_ID}))
  ## If external service type is LoadBalancer and distinct is true:
  # "advertised.listeners": |-
  #   EXTERNAL://kafka-$((${KAFKA_NODE_ID})).cluster.local:19092
  ## If external service type is LoadBalancer and distinct is false:
  # "advertised.listeners": |-
  #   EXTERNAL://${LOAD_BALANCER_IP}:31090
  ## Uncomment to define the EXTERNAL Listener protocol
  # "listener.security.protocol.map": |-
  #   PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT

  ## SSL/TLS encryption
  ## Documentation: https://kafka.apache.org/documentation/#security
  ## advertised.listeners here are appended to the defaults, not overwritten
  # advertised.listeners: "TLS://kafka-headless.default.svc.cluster.local:9094"
  # listener.security.protocol.map: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,TLS:SSL"
  # listeners: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,TLS://0.0.0.0:9094"
  # security.protocol: "ssl"
  # ssl.client.auth: "none"
  #
  ## Options to enable SSL using PEM format
  ## Provide either ssl.keystore.location & ssl.truststore.location
  ## or ssl.keystore.key & ssl.keystore.certificate.chain
  # ssl.keystore.certificate.chain: |
  #   -----BEGIN CERTIFICATE-----\  # server cert
  #   MIIEyDCCAzCgAwIBAgIRAO8MGa5Gv9KBRb7D0Rcyj24wDQYJKoZIhvcNAQELBQAw\
  #   ...
  #   xnwiB0bI7V6Kf5KVgisZDtJF8APTfHy4ey3mXA==\
  #   -----END CERTIFICATE-----\ # intermediate certs would come next
  #   -----BEGIN CERTIFICATE-----\ # root cert
  #   MIIFJzCCA4+gAwIBAgIQX6DM59eAmZLzzdoyD0jbtDANBgkqhkiG9w0BAQsFADCB\
  #   ...
  #   R4eNRWMls7ceteGynZLUL0LULwW8Wio8w3Ht\
  #   -----END CERTIFICATE-----
  # ssl.keystore.key: |
  #   -----BEGIN PRIVATE KEY-----\
  #   MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCyI8ugVF4Og0Os\
  #   ...
  #   AdkIu2G1Xt6aidSQX8P5lxE1XadgO8b1JC8v86W4wzXnIZXZP/PE5U3w6eDdAI9w\
  #   X3ENvVkJ02bIDoRJ1oHchbHA\
  #   -----END PRIVATE KEY----
  # ssl.keystore.location: "/etc/kafka/secrets/kafka-keypair.pem" # key followed by cert
  # ssl.keystore.type: "PEM"
  # ssl.truststore.location: "/etc/kafka/secrets/CAs/rootCA.pem"
  # ssl.truststore.type: "PEM"
  # inter.broker.listener.name: "PLAINTEXT"
  # security.inter.broker.protocol: "PLAINTEXT"

# -- Add additional Environment Variables in the dictionary format.
#   key: "value"
envOverrides: {}

# -- A collection of additional ports to expose on brokers (formatted as normal containerPort yaml).
# Useful when the image exposes metrics (like prometheus, etc.) through a javaagent instead of a sidecar.
additionalPorts: {}

## Persistence configuration. Specify if and how to persist data to a persistent volume.
##
persistence:
  # -- Use a PVC to persist data
  enabled: true

  # -- The size of the PersistentVolume to allocate to each Kafka Pod in the StatefulSet. For
  # production servers this number should likely be much larger.
  size: "1Gi"

  # -- The location within the Kafka container where the PV will mount its storage and Kafka will
  # store its logs.
  mountPath: "/opt/kafka/data"

  ## Kafka data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If set to null (the default) or undefined, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  # -- Storage class of backing PVC
  storageClass: null

jmx:
  ## Rules to apply to the Prometheus JMX Exporter.  Note while lots of stats have been cleaned and exposed,
  ## there are still more stats to clean up and expose, others will never get exposed.  They keep lots of duplicates
  ## that can be derived easily.  The configMap in this chart cleans up the metrics it exposes to be in a Prometheus
  ## format, eg topic, broker are labels and not part of metric name. Improvements are gladly accepted and encouraged.
  configMap:
    # -- Enable the default ConfigMap for JMX, note a configMap is needed
    enabled: true

    ## To allow all metrics through (warning its crazy excessive) comment out below `overrideConfig` and set
    ## `whitelistObjectNames: []`
    # -- Allows config file to be generated by passing values to ConfigMap
    # @default -- see `values.yaml`
    overrideConfig:
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      ssl: false
      whitelistObjectNames:
        [
          "kafka.controller:*",
          "kafka.server:*",
          "java.lang:*",
          "kafka.network:*",
          "kafka.log:*",
          "jvm:*",
        ]
      rules:
        - pattern: kafka.server<type=(BrokerTopicMetrics), name=(BytesInPerSec|BytesOutPerSec|MessagesInPerSec|Failed.+), topic=(.+)><>Count
          name: kafka_server_$1_$2_total
          labels:
            topic: "$3"
          type: COUNTER

        # Node-level replica stats
        - pattern: kafka.server<type=(ReplicaManager), name=(UnderReplicatedPartitions|UnderMinIsrPartitionCount|OfflineReplicaCount|LeaderCount|PartitionCount)><>Value
          name: kafka_server_$1_$2_value
          type: GAUGE
        - pattern: kafka.server<type=(ReplicaManager), name=(IsrExpandsPerSec|IsrShrinksPerSec)><>Value
          name: kafka_server_$1_$2_total
          type: COUNTER

        # Node-level replica lag. This could also be tracked at partition-level granularity, but this would
        # get extremely expensive given the number of Kafka nodes and partitions
        - pattern: kafka.server<type=(ReplicaFetcherManager), name=(MaxLag), clientId=(Replica)><>Value
          name: kafka_server_$1_$2_value
          type: GAUGE

        # Request stats
        - pattern: kafka.network<type=(RequestMetrics), name=(RequestsPerSec|TotalTimeMs|RequestQueueTimeMs|LocalTimeMs|RemoteTimeMs|ResponseQueueTimeMs|ResponseSendTimeMs), request=(Produce|FetchConsumer|FetchFollower)><>Count
          name: kafka_network_$1_$2_total
          labels:
            request: "$3"
          type: COUNTER

        # Controller stats
        - pattern: kafka.controller<type=(ControllerStats), name=(LeaderElectionRateAndTimeMs|UncleanLeaderElectionsPerSec)><>Count
          name: kafka_controller_$1_$2_total
          type: COUNTER
        - pattern: kafka.controller<type=(KafkaController), name=(ActiveControllerCount|OfflinePartitionsCount)><>Value
          name: kafka_controller_$1_$2_value
          type: GAUGE

        - pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
          name: kafka_$1_$2_$3
          type: GAUGE
          labels:
            quantile: "0.$4"

        - pattern: kafka.log<type=Log, name=(.+), topic=(.+), partition=(.+)><>Value
          name: kafka_log_$1
          type: COUNTER
          labels:
            topic: "$2"
            partition: "$3"

    # -- Allows setting the name of the ConfigMap to be used
    overrideName: ""

  # -- The jmx port which JMX style metrics are exposed (note: these are not scrapeable by Prometheus)
  port: 5555

  remote:
    # -- Enable remote JMX access
    enabled: true
    # -- Block all remote access
    localOnly: false
    # -- Require ssl to connect remotely
    ssl: false
    # -- Require remote authentication
    authenticate: false

  ## JMX Whitelist Objects, can be set to control which JMX metrics are exposed.  Only whitelisted
  ## values will be exposed via JMX Exporter.  They must also be exposed via Rules.  To expose all metrics
  ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`
  ## (2) commented out above `overrideConfig`.
  # -- Allows setting which JMX objects you want to expose via JMX stats to JMX Exporter
  # @default -- see `values.yaml`
  whitelistObjectNames:  # []
    - kafka.controller:*
    - kafka.server:*
    - java.lang:*
    - kafka.network:*
    - kafka.log:*

## Prometheus Exporters / Metrics
##
prometheus:
  ## Prometheus JMX Exporter: exposes the majority of Kafka's metrics
  # NOTE: This jmx exporter has not been used, and the exporter running
  #       in the kafka broker container is the preferred method.
  jmx:
    # -- Whether or not to expose JMX metrics to Prometheus
    enabled: false

    # -- JMX exporter container image
    image: docker.io/solsson/kafka-prometheus-jmx-exporter@sha256

    # -- JMX exporter container image tag
    imageTag: 70852d19ab9182c191684a8b08ac831230006d82e65d1db617479ea27884e4e8

    # -- Interval at which Prometheus scrapes JMX metrics, note: only used by Prometheus Operator
    interval: 10s

    # -- Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 10s

    # -- JMX Exporter Port which exposes metrics in Prometheus format for scraping
    port: 5555

    # -- Allows setting resource limits for jmx sidecar container
    resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 1Gi
      # requests:
      #   cpu: 100m
      #   memory: 100Mi

  ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter
  kafka:
    # -- Whether or not to create a separate Kafka exporter
    enabled: false

    # -- Kafka Exporter container image
    image: docker.io/danielqsj/kafka-exporter

    # -- Kafka Exporter container image tag
    imageTag: v1.8.0

    # -- Interval at which Prometheus scrapes Kafka metrics, note: only used by Prometheus Operator
    interval: 10s

    # -- Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 10s

    # -- Kafka Exporter Port which exposes metrics in Prometheus format for scraping
    port: 9308

    # -- Allows setting resource limits for kafka-exporter pod
    resources: {}
    #      limits:
    #        cpu: 200m
    #        memory: 1Gi
    #      requests:
    #        cpu: 100m
    #        memory: 100Mi

    # -- Tolerations for nodes that have taints on them.
    # Useful if you want to dedicate nodes to just run kafka-exporter
    # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # tolerations:
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    # -- Pod scheduling preferences (by default keep pods within a release on separate nodes).
    # By default we don't set affinity.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    affinity: {}
    ## Alternatively, this typical example defines:
    ## affinity (to encourage Kafka Exporter pods to be collocated with Kafka pods)
    # affinity:
    #   podAffinity:
    #     preferredDuringSchedulingIgnoredDuringExecution:
    #      - weight: 50
    #        podAffinityTerm:
    #          labelSelector:
    #            matchExpressions:
    #            - key: app
    #              operator: In
    #              values:
    #                - kafka
    #          topologyKey: "kubernetes.io/hostname"

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}

  operator:
    # -- True if using the Prometheus Operator, False if not
    enabled: false

    serviceMonitor:
      # -- Namespace in which to install the ServiceMonitor resource. Default to kube-prometheus install.
      namespace: monitoring
      # -- Set namespace to release namespace
      releaseNamespace: false

      ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
      ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
      ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
      # -- Default to kube-prometheus install (CoreOS recommended), but should be set according to Prometheus install
      selector:
        prometheus: kube-prometheus

      # -- Relabel a target before scrape
      # @default -- see `values.yaml`
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: instance
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: kafka_node
        - sourceLabels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          regex: kafka-(.*)
          replacement: $1
          targetLabel: kafka_cluster
      # -- Relabel a metric before sample ingestion
      # @default -- see `values.yaml`
      metricRelabelings:
        - action: drop
          sourceLabels: [topic]
          regex: (test.*|temp.*|__consumer.*)
        - action: drop
          sourceLabels: [kafka_topic]
          regex: (test.*|temp.*|__consumer.*)
    prometheusRule:
      # -- True to create a PrometheusRule resource for Prometheus Operator, False if not
      enabled: false

      # -- Namespace in which to install the PrometheusRule resource. Default to kube-prometheus install.
      namespace: monitoring
      # -- Set namespace to release namespace
      releaseNamespace: false

      ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
      ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
      ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
      # -- Default to kube-prometheus install (CoreOS recommended), but should be set according to Prometheus install
      selector:
        prometheus: kube-prometheus

      # -- Define the prometheus rules. See values file for examples.
      rules: {}
      # - alert: KafkaNoActiveControllers
      #   annotations:
      #     message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is less than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
      #   expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) < 1
      #   for: 5m
      #   labels:
      #     severity: critical
      # - alert: KafkaMultipleActiveControllers
      #   annotations:
      #     message: The number of active controllers in {{ "{{" }} $labels.namespace {{ "}}" }} is greater than 1. This usually means that some of the Kafka nodes aren't communicating properly. If it doesn't resolve itself you can try killing the pods (one by one whilst monitoring the under-replicated partitions graph).
      #   expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by (namespace) > 1
      #   for: 5m
      #   labels:
      #     severity: critical

  ## Prometheus Java Agent JMX Exporter
  agent:
    # -- Whether or not to expose JMX metrics to Prometheus as a Java Agent (alternative to prometheus.jmx)
    enabled: false

    # -- JMX Agent Exporter version
    # https://github.com/prometheus/jmx_exporter/releases
    version: 1.1.0

    # -- Interval at which Prometheus scrapes JMX metrics, note: only used by Prometheus Operator
    interval: 30s

    # -- Timeout at which Prometheus timeouts scrape run, note: only used by Prometheus Operator
    scrapeTimeout: 30s

    # -- JMX Exporter Port which exposes metrics in Prometheus format for scraping
    port: 5557

    # -- Custom label selector
    selector: {}

## Kafka Config job configuration
##
configJob:
  # -- Specify the number of retries before considering kafka-config job as failed.
  # https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy
  backoffLimit: 6

## Topic creation and configuration.
## The job will be run on a deployment only when the config has been changed.
## - If 'partitions' and 'replicationFactor' are specified we create the topic (with --if-not-exists.)
## - If 'partitions', 'replicationFactor' and 'reassignPartitions' are specified we reassign the partitions to
## increase the replication factor of an existing topic.
## - If 'partitions' is specified we 'alter' the number of partitions. This will
## silently and safely fail if the new setting isn’t strictly larger than the old (i.e. a NOOP.) Do be aware of the
## implications for keyed topics (ref: https://docs.confluent.io/current/kafka/post-deployment.html#admin-operations)
## - If 'defaultConfig' is specified it's deleted from the topic configuration. If it isn't present,
## it will silently and safely fail.
## - If 'config' is specified it's added to the topic configuration.
##
## Note: To increase the 'replicationFactor' of a topic, 'reassignPartitions' must be set to true (see above).
##
# -- List of topics to create & configure. Can specify name, partitions, replicationFactor, reassignPartitions, config. See values.yaml
# **WARNING**: this feature has not been implemented
topics: []
  # - name: myExistingTopicConfig
  #   config: "cleanup.policy=compact,delete.retention.ms=604800000"
  # - name: myExistingTopicReassignPartitions
  #   partitions: 8
  #   replicationFactor: 5
  #   reassignPartitions: true
  # - name: myExistingTopicPartitions
  #   partitions: 8
  # - name: myNewTopicWithConfig
  #   partitions: 8
  #   replicationFactor: 3
  #   defaultConfig: "segment.bytes,segment.ms"
  #   config: "cleanup.policy=compact,delete.retention.ms=604800000"
  # - name: myAclTopicPartitions
  #   partitions: 8
  #   acls:
  #     - user: read
  #       operations: [ Read ]
  #     - user: read_and_write
  #       operations:
  #         - Read
  #         - Write
  #     - user: all
  #       operations: [ All ]

# -- Enable/disable the chart's tests.  Useful if using this chart as a dependency of
# another chart and you don't want these tests running when trying to develop and
# test your own chart.
testsEnabled: true
